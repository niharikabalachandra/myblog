{
  "hash": "94c6cdde982251db3adfb4f633686316",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Image Recognition Engine with GenAI Explanation\nauthor: Niharika Balachandra\ndate: '2024-02-03'\ncategories:\n  - code\n  - analysis\n  - deploy\nimage: image.jpg\ndraft: false\nformat:\n  html:\n    code-fold: true\n    toc: true\n    toc-location: left\n---\n\n# The Deployed Application\n<iframe\n\tsrc=\"https://niharikabalachandra-find-and-explain-flowers-streamlit.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"850\"\n\theight=\"450\"\n></iframe>\n\n# Introduction: Unpacking the Application\nI wanted to develop an Image Recognition Engine that not only identifies images but also offers an initial explanation of the recognized image using an open-source, large language model. The ultimate goal was to create a self-contained application that goes beyond image recognition and also serves as a starting point to gain an understanding of what an identified image is all about. \n\nThis particular application shown above focuses on recognition of flowers and is based on the [102 Category Flower Dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html)\n\nThere are 3 main components to the app and I explain each component in more detail in the following sections: \n- Image Recognition\n- Generating Image Explanation via LLM\n- Streamlit Application \n\n# Image Recognition\n\n## Model Tuning \nThe idea is to use transfer learning to fine tune a pre-trained vision model. I use the Resnet50 pre-trained model for this project but there are a host of other pre-trained vision models you can find via [pytorch-image-models or timm](https://github.com/huggingface/pytorch-image-models?tab=readme-ov-file#models). The easiest way to fine tune the Resnet50 pre-trained model is using the [fastai library](https://timm.fast.ai/#Fine-tune-timm-model-in-fastai). Here is the [notebook](https://www.kaggle.com/code/niharikabalachandra/flowerdetector-oxford-102-flowers\n) I used to generate a pickled version of the fine-tuned model. \n\n## Deploying on Hugging Face \nWe now needed to host this serialized model on Hugging Face and interact with it via the huggingface_hub python library. Uploading a model to Hugging Face is really simple via the UI and this [doc](https://huggingface.co/docs/hub/en/models-uploading#using-the-web-interface) walks though the process in detail. The following code can then be used to load this hosted model into memory so we can build the app around it.\n\n::: {#e2007ab0 .cell execution_count=1}\n``` {.python .cell-code}\nfrom huggingface_hub import hf_hub_download\n\nREPO_ID = \"username/huggingface_repo\"\nFILENAME = \"export.pkl\"\n\nmodel_file_name = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n```\n:::\n\n\n## Architecture\n\n\n```{mermaid}\n---\ntitle: Model Fine-tuning\n---\nflowchart LR\n  A(Resnet50 Pre-trained Model) --> B(Fine-tuned for Identification of Flowers)\n  B --> C(Serialized Model)\n```\n\n```{mermaid}\n---\ntitle: Deploying Model on Hugging Face \n---\nflowchart LR\n C(Serialized Fine-tuned Model) --> D(Host Serialized Model on Hugging Face)\nD --> E(Load Model as needed)\n```\n\n::: {#6035f0f4 .cell execution_count=2}\n``` {.python .cell-code}\nfrom fastai.vision.all import *\nfrom huggingface_hub import hf_hub_download\n\nREPO_ID = \"<hugging_face_repo_hosting_fine-tuned_model>\"\nFILENAME = \"export.pkl\"\n\nmodel_file_name = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\nlearn_inf = load_learner(model_file_name)\nlabels = learn_inf.dls.vocab\n\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn_inf.predict(img)\n    dict_ = {labels[i]: float(probs[i]) for i in range(len(labels))}\n    dict_ = dict(sorted(dict_.items(), key=lambda x: x[1], reverse=True))\n    best_class = list(dict_.keys())[0]\n    return dict_ , best_class\n\nimage = PILImage.create(file_name)\ncol1.image(image, use_column_width=True)\npredictions, best_class = predict(image)\n```\n:::\n\n\n# Generating Image Explanation via LLM\n\n## Deploying Quantized LLM on Hugging Face\nThe idea here is to host a dockerized version of a quantized open source LLM using the CPU tier on Hugging Face Spaces and exposing the LLM via an API endpoint for us to then be able to interact with for free. I chose to use [Zephyr 7B Alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha) which is a fine-tuned version of Mistral AI's [Mistral-7B-v0.1](mistralai/Mistral-7B-v0.1).\n\nMy implementation of this step is inspired by this [blog post](https://gathnex.medium.com/how-to-deploy-llm-for-free-of-cost-6e7947d9b64a) that goes over the exact process of getting to an API endpoint.\n\n## Architecture\n\n\n```{mermaid}\n---\ntitle: Deploying Quantized LLM on Hugging Face\n---\nflowchart LR\nC(FastAPI Function for LLM completion) --> D(Containerize our Application)\nD --> E(Deploy via Hugging Face Spaces)\nE --> F(Fast API Endpoint)\n```\n\n::: {#c958a8eb .cell execution_count=3}\n``` {.python .cell-code}\nimport requests\nimport os\n\n# hugging_face_token_with_read_access is stored as a hugging face secret\nHF_TOKEN = os.environ.get('<hugging_face_token_with_read_access>')\nbest_class = '<identified_image_label>'\nAPI_URL = \"<API_endpoint_of_hugging-face_hosted_LLM-model>\"\n\nrequest_auth = f'Bearer {HF_TOKEN}' \nheaders = {\n    'accept': 'application/json',\n    'Content-Type': 'application/json',\n    'Authorization': request_auth\n}\njson_data = {\n    'prompt': f'what is a {best_class} in 2 sentences?'\n}\n\nresponse = requests.post(\n    API_URL,\n    headers=headers,\n    json=json_data,\n)\n\ntry:\n    respose_txt = response.json()\nexcept Exception as e:\n    print(f'Error details: {e}')\n```\n:::\n\n\n# Streamlit Application \n## Deploying Streamlit Application on Hugging Face Spaces\nI start with creating a new [Streamlit Hugging Face Space](https://huggingface.co/docs/hub/en/spaces-sdks-streamlit#create-a-new-streamlit-space). I then build the streamlit app that integrates all the 3 components: load in the image recognition model, build out the interface to upload images and display results and finally to pass the prediction via a prompt to generate a high-level explanation of the flower species identified. You can refer to the [app.py](https://huggingface.co/spaces/niharikabalachandra/find_and_explain_flowers_streamlit/blob/main/app.py) file for more information on how everything was stitched together in an application.\n\n## Architecture\n\n\n```{mermaid}\n---\ntitle: Streamlit Application on Hugging Space Spaces\n---\nflowchart LR\n\nD(Build App User Interface)\nD --> B(Upload Flower Image)\nD --> C(Load Image Recognition Model)\nB --> C\nC --> E{Identify Flower Species}\nE --> N(Output Identified Flower Species)\n\nE --> F(Pass Identified Label to LLM)\nF --> G(Generate LLM Response)\nG --> O(Ouput Response on Identified Flower Species)\nO --> D\n```\n\n\n# Reference\n#### Model tuning | Image Recognition\n- My approach was inspired by this kaggle [notebook](https://www.kaggle.com/code/hobaak/flowerdetector-oxford-102-flowers)\n\n#### Deploying Quantized LLM on Hugging Face | Generating Image Explanation via LLM\n- The [blog post](https://gathnex.medium.com/how-to-deploy-llm-for-free-of-cost-6e7947d9b64a) that goes over the exact process of getting to an API endpoint\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}